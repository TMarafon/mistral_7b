{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNyeU5jKKgy8lo0FXSmyI8r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TMarafon/mistral_7b/blob/main/Mistral_7B_Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Doing curl requests\n",
        "In this notebook, we'll experiment with the instruct model, as it is trained for instructions. As per the model card, the expected format for a prompt is as follows\n",
        "\n",
        "From the model card\n",
        "\n",
        "In order to leverage instruction fine-tuning, your prompt should be surrounded by [INST] and [\\INST] tokens. The very first instruction should begin with a begin of sentence id. The next instructions should not. The assistant generation will be ended by the end-of-sentence token id.\n",
        "\n",
        "<s>[INST] {{ user_msg_1 }} [/INST] {{ model_answer_1 }}</s> [INST] {{ user_msg_2 }} [/INST] {{ model_answer_2 }}</s>\n",
        "\n",
        "We'll start an initial query without prompt formatting, which works ok for simple queries."
      ],
      "metadata": {
        "id": "0A63p27l10Dp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "YW31sWcX4wBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "gjtEoLtZ42Sp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1 \\\n",
        "  --header \"Content-Type: application/json\" \\\n",
        "\t-X POST \\\n",
        "\t-d '{\"inputs\": \"Explain ML as a pirate\", \"parameters\": {\"max_new_tokens\": 50}}' \\\n",
        "\t-H \"Authorization: Bearer Your_token\" #hugging_face api token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eJmXcxU2C2H",
        "outputId": "a93bef26-6985-4c56-a1a4-fcbb0a34aa01"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{\"generated_text\":\"Explain ML as a pirate.\\n\\nML is like a treasure map for pirates. Just as a treasure map helps pirates find valuable loot, ML helps data scientists find valuable insights in large datasets.\\n\\nPirates use their knowledge of the ocean and their\"}]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Programmatic usage with Python\n",
        "\n",
        "You can do simple requests, but the huggingface_hub library provides nice utilities to easily use the model. Among the things we can use are:\n",
        "\n",
        "InferenceClient and AsyncInferenceClient to perform inference either in a sync or async way.\n",
        "Token streaming: Only load the tokens that are needed\n",
        "Easily configure generation params, such as temperature, nucleus sampling (top-p), repetition penalty, stop sequences, and more.\n",
        "Obtain details of the generation (such as the probability of each token or whether a token is the last token)."
      ],
      "metadata": {
        "id": "6ocFShjR2fME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install huggingface_hub gradio"
      ],
      "metadata": {
        "id": "gWGKG3s62R09"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "client = InferenceClient(\n",
        "    model=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
        "    token=\"\" #hugging_face api token\n",
        ")\n",
        "\n",
        "prompt = \"\"\"<s>[INST] What is your favourite condiment?  [/INST]</s>\n",
        "\"\"\"\n",
        "\n",
        "res = client.text_generation(prompt, max_new_tokens=95)\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bM7eqwW_2z1H",
        "outputId": "14e71e79-1c9b-42fd-d6f7-35f72f9c06ab"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "I don't have personal experiences or preferences. However, I can tell you that people's favorite condiments can vary widely based on personal taste and cultural background. Some popular condiments include ketchup, mustard, mayonnaise, hot sauce, soy sauce, and olive oil.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res = client.text_generation(prompt, max_new_tokens=35, stream=True, details=True, return_full_text=False)\n",
        "for r in res: # this is a generator\n",
        "  # print the token for example\n",
        "  print(r.token.text)\n",
        "  continue"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_qLqK1h3vhj",
        "outputId": "4a96795e-fcf6-412c-eb59-ff085038e704"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "I\n",
            " don\n",
            "'\n",
            "t\n",
            " have\n",
            " personal\n",
            " experiences\n",
            " or\n",
            " preferences\n",
            ".\n",
            " However\n",
            ",\n",
            " I\n",
            " can\n",
            " tell\n",
            " you\n",
            " that\n",
            " people\n",
            "'\n",
            "s\n",
            " favorite\n",
            " cond\n",
            "iments\n",
            " can\n",
            " vary\n",
            " widely\n",
            " based\n",
            " on\n",
            " personal\n",
            " taste\n",
            " and\n",
            " cultural\n",
            " background\n",
            ".\n"
          ]
        }
      ]
    }
  ]
}